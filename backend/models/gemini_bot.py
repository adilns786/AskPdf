from google.generativeai import GenerativeModel, configure
from typing import Optional, Dict, Any


def answer_pdf_question(
    pdf_content: str,
    question: str,
    api_key: str,
    model: str = "gemini-pro",
    temperature: float = 0.7,
    max_tokens: Optional[int] = 500,
    safety_settings: Optional[Dict[str, Any]] = None
) -> str:
    """
    Answers a user's question based on the context of a PDF content using the Gemini API.

    Args:
        pdf_content (str): Text content extracted from the PDF document.
        question (str): User's question to be answered in the context of the PDF.
        api_key (str): Your Gemini API key.
        model (str): The model to use (default: "gemini-pro").
        temperature (float): Controls randomness in the output (0.0 to 1.0).
        max_tokens (int, optional): Maximum number of tokens in the response.
        safety_settings (dict, optional): Custom safety settings for content filtering.

    Returns:
        str: The answer generated by Gemini in the context of the PDF content.

    Raises:
        Exception: If API call fails or configuration is invalid.
    """
    # Construct the template prompt
    template_prompt = f"""
    You are an intelligent assistant capable of answering specific questions based on the provided document text.

    Document text:
    ---
    {pdf_content}
    ---

    Question:
    {question}

    Provide a clear, concise answer based on the document context,in plain text with no bold or italic or any formatting.
    """
    
    try:
        # Configure the Gemini API
        configure(api_key=api_key)
        
        # Initialize the model
        gemini_model = GenerativeModel(model_name=model)
        
        # Prepare generation configuration
        generation_config = {
            "temperature": temperature,
        }
        if max_tokens:
            generation_config["max_output_tokens"] = max_tokens
            
        # Set safety settings if provided
        if safety_settings:
            gemini_model.safety_settings = safety_settings
            
        # Generate the answer
        response = gemini_model.generate_content(
            template_prompt,
            generation_config=generation_config
        )
        
        return response.text.strip()
    
    except Exception as e:
        raise Exception(f"Error generating answer with Gemini API: {str(e)}")

def analyze_pdf_content(
    pdf_text: str,
    action: str,
    api_key: str,
    model: str = "gemini-pro",
    temperature: float = 0.7,
    max_tokens: Optional[int] = 500,
    safety_settings: Optional[Dict[str, Any]] = None
) -> str:
    """
    Analyze PDF content using Gemini API with a specified action like 'summarize' or 'insight'.
    
    Args:
        pdf_text (str): Text extracted from the PDF document.
        action (str): The action to perform (e.g., 'summarize', 'insight').
        api_key (str): Your Gemini API key.
        model (str): The model to use (default: "gemini-pro").
        temperature (float): Controls randomness in the output (0.0 to 1.0).
        max_tokens (int, optional): Maximum number of tokens in the response.
        safety_settings (dict, optional): Custom safety settings for content filtering.
    
    Returns:
        str: The generated response from Gemini.
    
    Raises:
        Exception: If API call fails or configuration is invalid.
    """
    # Construct the template prompt
    template_prompt = f"""
    You are a highly capable PDF analysis assistant embedded in a website. Users can upload documents, and your job is to provide actionable insights based on their input request.
    
    The user has provided the following document text:
    ---
    {pdf_text}
    ---
    
    The requested action is: {action}.
    Please provide your response based on this request,in plain text with no bold or italic or any formatting.
    """
    
    try:
        # Configure the Gemini API
        configure(api_key=api_key)
        
        # Get the specified model
        model = GenerativeModel(model_name=model)
        
        # Prepare generation config
        generation_config = {
            "temperature": temperature,
        }
        if max_tokens:
            generation_config["max_output_tokens"] = max_tokens
            
        # Set safety settings if provided
        if safety_settings:
            model.safety_settings = safety_settings
            
        # Generate content
        response = model.generate_content(
            template_prompt,
            generation_config=generation_config
        )
        
        return response.text
    
    except Exception as e:
        raise Exception(f"Error analyzing PDF content with Gemini API: {str(e)}")
